{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AkHQ: http://localhost:8082  \n",
    "Spark-Master http://localhost:8083  \n",
    "Spark-Worker-1 http://localhost:8084\n",
    "\n",
    "https://github.com/aehrc/pathling/tree/issue/452/lib/python#python-api-for-pathling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Connect To Test FHIR Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n",
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "# contributor license agreements.  See the NOTICE file distributed with\n",
      "# this work for additional information regarding copyright ownership.\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "# (the \"License\"); you may not use this file except in compliance with\n",
      "# the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "# included in all the spark scripts with source command\n",
      "# should not be executable directly\n",
      "# also should not be passed any arguments, since we need original $*\n",
      "\n",
      "# symlink and absolute path should rely on SPARK_HOME to resolve\n",
      "if [ -z \"${SPARK_HOME}\" ]; then\n",
      "  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n",
      "fi\n",
      "\n",
      "export SPARK_CONF_DIR=\"${SPARK_CONF_DIR:-\"${SPARK_HOME}/conf\"}\"\n",
      "# Add the PySpark classes to the PYTHONPATH:\n",
      "if [ -z \"${PYSPARK_PYTHONPATH_SET}\" ]; then\n",
      "  export PYTHONPATH=\"${SPARK_HOME}/python:${PYTHONPATH}\"\n",
      "  export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:${PYTHONPATH}\"\n",
      "  export PYSPARK_PYTHONPATH_SET=1\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!cat /usr/local/bin/before-notebook.d/spark-config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'SHELL': '/bin/bash',\n",
       "        'HOSTNAME': '08b42fc39e7e',\n",
       "        'LANGUAGE': 'en_US.UTF-8',\n",
       "        'SPARK_OPTS': '--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info',\n",
       "        'NB_UID': '1000',\n",
       "        'PWD': '/home/jovyan',\n",
       "        'PYSPARK_SUBMIT_ARGS': '     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,au.csiro.pathling:encoders:5.0.3-SNAPSHOT     --repositories https://oss.sonatype.org/content/repositories/snapshots     pyspark-shell',\n",
       "        'HOME': '/home/jovyan',\n",
       "        'LANG': 'en_US.UTF-8',\n",
       "        'NB_GID': '100',\n",
       "        'XDG_CACHE_HOME': '/home/jovyan/.cache/',\n",
       "        'APACHE_SPARK_VERSION': '3.2.1',\n",
       "        'PYTHONPATH': '/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip:/usr/local/spark/python:',\n",
       "        'HADOOP_VERSION': '3.2',\n",
       "        'SHLVL': '0',\n",
       "        'CONDA_DIR': '/opt/conda',\n",
       "        'SPARK_HOME': '/usr/local/spark',\n",
       "        'SPARK_CONF_DIR': '/usr/local/spark/conf',\n",
       "        'NB_USER': 'jovyan',\n",
       "        'LC_ALL': 'en_US.UTF-8',\n",
       "        'PATH': '/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin',\n",
       "        'PYSPARK_PYTHONPATH_SET': '1',\n",
       "        'DEBIAN_FRONTEND': 'noninteractive',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_PARENT_PID': '8',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"Kafka, Spark and FHIR Data\"\n",
    "master = \"spark://spark-master:7077\"\n",
    "#master = \"local[*]\"\n",
    "kafka_topic = \"fhir.post-gateway-kdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathling.etc import find_jar\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(appName) \\\n",
    "        .master(master) \\\n",
    "        .config('spark.jars', find_jar()) \\\n",
    "        .config('spark.ui.port','0') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.host', '08b42fc39e7e'), ('spark.app.initial.jar.urls', 'spark://08b42fc39e7e:38271/jars/encoders-5.0.3-SNAPSHOT-all.jar'), ('spark.submit.pyFiles', '/home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar,/home/jovyan/.ivy2/jars/au.csiro.pathling_encoders-5.0.3-SNAPSHOT.jar,/home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar,/home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar,/home/jovyan/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/home/jovyan/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,/home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar,/home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,/home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,/home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar,/home/jovyan/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar,/home/jovyan/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,/home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-base-5.7.2.jar,/home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-structures-r4-5.7.2.jar,/home/jovyan/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.13.2.jar,/home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.2.1.jar,/home/jovyan/.ivy2/jars/org.apache.commons_commons-lang3-3.12.0.jar,/home/jovyan/.ivy2/jars/org.apache.commons_commons-text-1.9.jar,/home/jovyan/.ivy2/jars/commons-codec_commons-codec-1.15.jar,/home/jovyan/.ivy2/jars/commons-io_commons-io-2.11.0.jar,/home/jovyan/.ivy2/jars/com.google.guava_guava-31.0.1-jre.jar,/home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.36.jar,/home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.2.jar,/home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.2.jar,/home/jovyan/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,/home/jovyan/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,/home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.12.0.jar,/home/jovyan/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.7.1.jar,/home/jovyan/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,/home/jovyan/.ivy2/jars/com.squareup.okhttp3_okhttp-3.14.9.jar,/home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.utilities-5.6.27.jar,/home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.r4-5.6.27.jar,/home/jovyan/.ivy2/jars/com.squareup.okio_okio-1.17.2.jar'), ('spark.app.initial.file.urls', 'spark://08b42fc39e7e:38271/files/org.xerial.snappy_snappy-java-1.1.8.4.jar,spark://08b42fc39e7e:38271/files/org.slf4j_slf4j-api-1.7.36.jar,spark://08b42fc39e7e:38271/files/org.apache.kafka_kafka-clients-2.8.0.jar,spark://08b42fc39e7e:38271/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar,spark://08b42fc39e7e:38271/files/com.google.guava_guava-31.0.1-jre.jar,spark://08b42fc39e7e:38271/files/com.fasterxml.jackson.core_jackson-core-2.13.2.jar,spark://08b42fc39e7e:38271/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar,spark://08b42fc39e7e:38271/files/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.13.2.jar,spark://08b42fc39e7e:38271/files/ca.uhn.hapi.fhir_org.hl7.fhir.utilities-5.6.27.jar,spark://08b42fc39e7e:38271/files/org.apache.htrace_htrace-core4-4.1.0-incubating.jar,spark://08b42fc39e7e:38271/files/ca.uhn.hapi.fhir_org.hl7.fhir.r4-5.6.27.jar,spark://08b42fc39e7e:38271/files/au.csiro.pathling_encoders-5.0.3-SNAPSHOT.jar,spark://08b42fc39e7e:38271/files/ca.uhn.hapi.fhir_hapi-fhir-base-5.7.2.jar,spark://08b42fc39e7e:38271/files/org.checkerframework_checker-qual-3.12.0.jar,spark://08b42fc39e7e:38271/files/com.fasterxml.jackson.core_jackson-annotations-2.13.2.jar,spark://08b42fc39e7e:38271/files/commons-io_commons-io-2.11.0.jar,spark://08b42fc39e7e:38271/files/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar,spark://08b42fc39e7e:38271/files/com.squareup.okhttp3_okhttp-3.14.9.jar,spark://08b42fc39e7e:38271/files/org.lz4_lz4-java-1.7.1.jar,spark://08b42fc39e7e:38271/files/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,spark://08b42fc39e7e:38271/files/org.spark-project.spark_unused-1.0.0.jar,spark://08b42fc39e7e:38271/files/com.google.errorprone_error_prone_annotations-2.7.1.jar,spark://08b42fc39e7e:38271/files/ca.uhn.hapi.fhir_hapi-fhir-structures-r4-5.7.2.jar,spark://08b42fc39e7e:38271/files/com.google.j2objc_j2objc-annotations-1.3.jar,spark://08b42fc39e7e:38271/files/org.apache.commons_commons-text-1.9.jar,spark://08b42fc39e7e:38271/files/com.fasterxml.jackson.core_jackson-databind-2.13.2.1.jar,spark://08b42fc39e7e:38271/files/org.apache.hadoop_hadoop-client-api-3.3.1.jar,spark://08b42fc39e7e:38271/files/org.apache.commons_commons-lang3-3.12.0.jar,spark://08b42fc39e7e:38271/files/commons-logging_commons-logging-1.1.3.jar,spark://08b42fc39e7e:38271/files/org.apache.commons_commons-pool2-2.6.2.jar,spark://08b42fc39e7e:38271/files/commons-codec_commons-codec-1.15.jar,spark://08b42fc39e7e:38271/files/com.google.code.findbugs_jsr305-3.0.0.jar,spark://08b42fc39e7e:38271/files/com.squareup.okio_okio-1.17.2.jar,spark://08b42fc39e7e:38271/files/com.google.guava_failureaccess-1.0.1.jar'), ('spark.jars', '/opt/conda/lib/python3.9/site-packages/pathling/jars/encoders-5.0.3-SNAPSHOT-all.jar'), ('spark.executor.id', 'driver'), ('spark.driver.port', '38271'), ('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true'), ('spark.app.name', 'Kafka, Spark and FHIR Data'), ('spark.app.startTime', '1652356085761'), ('spark.ui.port', '0'), ('spark.app.id', 'app-20220512114807-0001'), ('spark.rdd.compress', 'True'), ('spark.master', 'spark://spark-master:7077'), ('spark.files', 'file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar,file:///home/jovyan/.ivy2/jars/au.csiro.pathling_encoders-5.0.3-SNAPSHOT.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar,file:///home/jovyan/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar,file:///home/jovyan/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-base-5.7.2.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-structures-r4-5.7.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.2.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-lang3-3.12.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-text-1.9.jar,file:///home/jovyan/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/jovyan/.ivy2/jars/commons-io_commons-io-2.11.0.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_guava-31.0.1-jre.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.36.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.12.0.jar,file:///home/jovyan/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.7.1.jar,file:///home/jovyan/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///home/jovyan/.ivy2/jars/com.squareup.okhttp3_okhttp-3.14.9.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.utilities-5.6.27.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.r4-5.6.27.jar,file:///home/jovyan/.ivy2/jars/com.squareup.okio_okio-1.17.2.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/home/jovyan/work/spark-warehouse'), ('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true'), ('spark.ui.showConsoleProgress', 'true'), ('spark.repl.local.jars', 'file:///opt/conda/lib/python3.9/site-packages/pathling/jars/encoders-5.0.3-SNAPSHOT-all.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar,file:///home/jovyan/.ivy2/jars/au.csiro.pathling_encoders-5.0.3-SNAPSHOT.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar,file:///home/jovyan/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar,file:///home/jovyan/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-base-5.7.2.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_hapi-fhir-structures-r4-5.7.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.2.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-lang3-3.12.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-text-1.9.jar,file:///home/jovyan/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/jovyan/.ivy2/jars/commons-io_commons-io-2.11.0.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_guava-31.0.1-jre.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.36.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.2.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///home/jovyan/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.12.0.jar,file:///home/jovyan/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.7.1.jar,file:///home/jovyan/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///home/jovyan/.ivy2/jars/com.squareup.okhttp3_okhttp-3.14.9.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.utilities-5.6.27.jar,file:///home/jovyan/.ivy2/jars/ca.uhn.hapi.fhir_org.hl7.fhir.r4-5.6.27.jar,file:///home/jovyan/.ivy2/jars/com.squareup.okio_okio-1.17.2.jar')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "# default for startingOffsets is \"latest\", but \"earliest\" allows rewind for missed alerts    \n",
    "df = spark \\\n",
    "  .readStream  \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka1:19092\") \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,au.csiro.pathling:encoders:5.0.3-SNAPSHOT     --repositories https://oss.sonatype.org/content/repositories/snapshots     pyspark-shell'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"PYSPARK_SUBMIT_ARGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Writing job aborted\n=== Streaming Query ===\nIdentifier: gettable [id = 2d3b6015-366c-49e6-8bf4-f03d9ed3d075, runId = 5af4f65a-4400-4870-8e85-3ba1d8551086]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[fhir.post-gateway-kdb]]: {\"fhir.post-gateway-kdb\":{\"0\":5}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@230d7569\n+- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]\n   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@66ce5ab, KafkaV2[Subscribe[fhir.post-gateway-kdb]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      2\u001b[0m           \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m           \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgettable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# close connection after 30 seconds\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:99\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout must be a positive integer or float. Got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout)\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Writing job aborted\n=== Streaming Query ===\nIdentifier: gettable [id = 2d3b6015-366c-49e6-8bf4-f03d9ed3d075, runId = 5af4f65a-4400-4870-8e85-3ba1d8551086]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[fhir.post-gateway-kdb]]: {\"fhir.post-gateway-kdb\":{\"0\":5}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@230d7569\n+- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]\n   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@66ce5ab, KafkaV2[Subscribe[fhir.post-gateway-kdb]]\n"
     ]
    }
   ],
   "source": [
    "query = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "          .writeStream \\\n",
    "          .queryName(\"gettable\") \\\n",
    "          .format(\"memory\") \\\n",
    "          .start()\n",
    "\n",
    "# close connection after 30 seconds\n",
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_data = spark.sql(\"select * from gettable\")\n",
    "kafka_data.show()\n",
    "type(kafka_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = kafka_data.toPandas()\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Pathling into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathling.r4 import bundles\n",
    "\n",
    "resources = bundles.from_json(kafka_data, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = bundles.extract_entry(spark, resources, 'Patient')\n",
    "encounter = bundles.extract_entry(spark, resources, 'Encounter')\n",
    "condition = bundles.extract_entry(spark, resources, 'Condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.select(\"id\", \"birthDate\", \"gender\", \"address.postalCode\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter.select(\"id\", \"subject.reference\", \"serviceType.coding.code\", \"period.start\", \"period.end\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition.select(\"id\", \"encounter.reference\", \"code.coding.code\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
